{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1438e792",
   "metadata": {},
   "source": [
    "# Text Feature Extraction Pipeline\n",
    "\n",
    "## Overview\n",
    "This Jupyter notebook implements comprehensive text analysis and feature extraction for multi-language video transcripts. It uses spaCy and textdescriptives to analyze linguistic features across 24 different languages.\n",
    "\n",
    "### Key Features\n",
    "- Multi-language support (24 languages)\n",
    "- Comprehensive text metrics extraction:\n",
    "  - Readability scores\n",
    "  - Linguistic complexity measures\n",
    "  - Text quality metrics\n",
    "  - POS tag distributions\n",
    "  - Dependency parsing features\n",
    "- Fallback handling for unsupported languages\n",
    "- Progress tracking and resumable processing\n",
    "\n",
    "### Prerequisites\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbc7a68d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "from pathlib import Path\n",
    "import spacy\n",
    "import pandas as pd\n",
    "import textdescriptives\n",
    "import webvtt\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35e305dd",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "### Language Models\n",
    "Requires spaCy language models for:\n",
    "- Major European languages (en, de, fr, es, etc.)\n",
    "- Asian languages (zh, ja, ko)\n",
    "- Nordic languages (da, sv, nb)\n",
    "- Eastern European languages (ru, uk, pl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "093a6d15",
   "metadata": {},
   "outputs": [],
   "source": [
    "!python -m spacy download ca_core_news_md\n",
    "!python -m spacy download zh_core_web_md\n",
    "!python -m spacy download hr_core_news_md\n",
    "!python -m spacy download da_core_news_md\n",
    "!python -m spacy download nl_core_news_md\n",
    "!python -m spacy download en_core_web_md\n",
    "!python -m spacy download fi_core_news_md\n",
    "!python -m spacy download fr_core_news_md\n",
    "!python -m spacy download de_core_news_md\n",
    "!python -m spacy download el_core_news_md\n",
    "!python -m spacy download it_core_news_md\n",
    "!python -m spacy download ja_core_news_md\n",
    "!python -m spacy download ko_core_news_md\n",
    "!python -m spacy download lt_core_news_md\n",
    "!python -m spacy download mk_core_news_md\n",
    "!python -m spacy download nb_core_news_md\n",
    "!python -m spacy download pl_core_news_md\n",
    "!python -m spacy download pt_core_news_md\n",
    "!python -m spacy download ro_core_news_md\n",
    "!python -m spacy download ru_core_news_md\n",
    "!python -m spacy download sl_core_news_md\n",
    "!python -m spacy download es_core_news_md\n",
    "!python -m spacy download sv_core_news_md\n",
    "!python -m spacy download uk_core_news_md"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4a48a3b",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "### Input/Output\n",
    "- Input: \n",
    "  - `Video_Transcriptions.csv`: Video transcripts\n",
    "  - `Detected_Language_Confident.csv`: Language detection results\n",
    "- Output: \n",
    "  - `processed_text_features.csv`: Extracted text features\n",
    "\n",
    "### Process Flow\n",
    "1. Load transcriptions and language detection data\n",
    "2. Match appropriate language models\n",
    "3. Extract comprehensive text features\n",
    "4. Handle unsupported languages with fallback model\n",
    "5. Save results with progress tracking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4f066474-e393-44ae-a61a-405b19c9e8f5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Video ID</th>\n",
       "      <th>transcript</th>\n",
       "      <th>language</th>\n",
       "      <th>confidence</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0-pwca91OCM</td>\n",
       "      <td>These pork and mango spring rolls are one of ...</td>\n",
       "      <td>en</td>\n",
       "      <td>0.999961</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2K0AHZh6OrM</td>\n",
       "      <td>What's up y'all, Forrest here. To start off t...</td>\n",
       "      <td>en</td>\n",
       "      <td>0.999187</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-ED-vjRCKaE</td>\n",
       "      <td>What's up guys, I'm RandomFrankP back with an...</td>\n",
       "      <td>en</td>\n",
       "      <td>0.998429</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>LsNg-KrFxCA</td>\n",
       "      <td>Let me go to Big Head Joe's for you. They hav...</td>\n",
       "      <td>en</td>\n",
       "      <td>0.998661</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1S0lygj3w84</td>\n",
       "      <td>Would you be willing to trade the outfit you ...</td>\n",
       "      <td>en</td>\n",
       "      <td>0.986416</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      Video ID                                         transcript language  \\\n",
       "0  0-pwca91OCM   These pork and mango spring rolls are one of ...       en   \n",
       "1  2K0AHZh6OrM   What's up y'all, Forrest here. To start off t...       en   \n",
       "2  -ED-vjRCKaE   What's up guys, I'm RandomFrankP back with an...       en   \n",
       "3  LsNg-KrFxCA   Let me go to Big Head Joe's for you. They hav...       en   \n",
       "4  1S0lygj3w84   Would you be willing to trade the outfit you ...       en   \n",
       "\n",
       "   confidence  \n",
       "0    0.999961  \n",
       "1    0.999187  \n",
       "2    0.998429  \n",
       "3    0.998661  \n",
       "4    0.986416  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load transcriptions\n",
    "Video_Transcriptions = pd.read_csv(\"Video_Transcriptions.csv\")\n",
    "# and detected language\n",
    "dlc = pd.read_csv(\"Detected_Language_Confident.csv\")\n",
    "\n",
    "df = Video_Transcriptions.merge(dlc, on='Video ID', how='left')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9119a9d-470b-4454-971b-a4a83325666d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Resuming from 189 previously processed videos.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.12/site-packages/textdescriptives/components/coherence.py:44: UserWarning: [W008] Evaluating Span.similarity based on empty vectors.\n",
      "  similarities.append(sent.similarity(sents[i + order]))\n",
      "/opt/conda/lib/python3.12/site-packages/textdescriptives/components/coherence.py:44: UserWarning: [W008] Evaluating Span.similarity based on empty vectors.\n",
      "  similarities.append(sent.similarity(sents[i + order]))\n",
      "  2%|▏         | 195/10956 [00:46<1:20:21,  2.23it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[38;5;3m⚠ Could not load lexeme probability table for language da. This will\n",
      "result in NaN values for perplexity and entropy.\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.12/site-packages/textdescriptives/components/coherence.py:44: UserWarning: [W008] Evaluating Span.similarity based on empty vectors.\n",
      "  similarities.append(sent.similarity(sents[i + order]))\n",
      "/opt/conda/lib/python3.12/site-packages/textdescriptives/components/coherence.py:44: UserWarning: [W008] Evaluating Span.similarity based on empty vectors.\n",
      "  similarities.append(sent.similarity(sents[i + order]))\n",
      "/opt/conda/lib/python3.12/site-packages/textdescriptives/components/coherence.py:44: UserWarning: [W008] Evaluating Span.similarity based on empty vectors.\n",
      "  similarities.append(sent.similarity(sents[i + order]))\n",
      "  2%|▏         | 206/10956 [02:10<14:22:59,  4.82s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[38;5;3m⚠ Could not load lexeme probability table for language da. This will\n",
      "result in NaN values for perplexity and entropy.\u001b[0m\n",
      "\u001b[38;5;3m⚠ Could not load lexeme probability table for language da. This will\n",
      "result in NaN values for perplexity and entropy.\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.12/site-packages/textdescriptives/components/coherence.py:44: UserWarning: [W008] Evaluating Span.similarity based on empty vectors.\n",
      "  similarities.append(sent.similarity(sents[i + order]))\n",
      "/opt/conda/lib/python3.12/site-packages/textdescriptives/components/coherence.py:44: UserWarning: [W008] Evaluating Span.similarity based on empty vectors.\n",
      "  similarities.append(sent.similarity(sents[i + order]))\n",
      "  2%|▏         | 208/10956 [02:14<10:47:13,  3.61s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[38;5;3m⚠ Could not load lexeme probability table for language da. This will\n",
      "result in NaN values for perplexity and entropy.\u001b[0m\n",
      "\u001b[38;5;3m⚠ Could not load lexeme probability table for language da. This will\n",
      "result in NaN values for perplexity and entropy.\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.12/site-packages/textdescriptives/components/coherence.py:44: UserWarning: [W008] Evaluating Span.similarity based on empty vectors.\n",
      "  similarities.append(sent.similarity(sents[i + order]))\n",
      "  2%|▏         | 213/10956 [02:50<20:42:24,  6.94s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[38;5;3m⚠ Could not load lexeme probability table for language it. This will\n",
      "result in NaN values for perplexity and entropy.\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.12/site-packages/textdescriptives/components/coherence.py:44: UserWarning: [W008] Evaluating Span.similarity based on empty vectors.\n",
      "  similarities.append(sent.similarity(sents[i + order]))\n",
      "  2%|▏         | 220/10956 [03:40<23:16:48,  7.81s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[38;5;3m⚠ Could not load lexeme probability table for language ru. This will\n",
      "result in NaN values for perplexity and entropy.\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.12/site-packages/textdescriptives/components/coherence.py:44: UserWarning: [W008] Evaluating Span.similarity based on empty vectors.\n",
      "  similarities.append(sent.similarity(sents[i + order]))\n",
      "/opt/conda/lib/python3.12/site-packages/textdescriptives/components/coherence.py:44: UserWarning: [W008] Evaluating Span.similarity based on empty vectors.\n",
      "  similarities.append(sent.similarity(sents[i + order]))\n",
      "  2%|▏         | 222/10956 [03:50<20:15:53,  6.80s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[38;5;3m⚠ Could not load lexeme probability table for language da. This will\n",
      "result in NaN values for perplexity and entropy.\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.12/site-packages/textdescriptives/components/coherence.py:44: UserWarning: [W008] Evaluating Span.similarity based on empty vectors.\n",
      "  similarities.append(sent.similarity(sents[i + order]))\n",
      "  2%|▏         | 227/10956 [04:23<22:06:33,  7.42s/it]"
     ]
    }
   ],
   "source": [
    "lang2model = {\n",
    "    \"ca\": \"ca_core_news_md\",\n",
    "    \"zh\": \"zh_core_web_md\",\n",
    "    \"hr\": \"hr_core_news_md\",\n",
    "    \"da\": \"da_core_news_md\",\n",
    "    \"nl\": \"nl_core_news_md\",\n",
    "    \"en\": \"en_core_web_md\",\n",
    "    \"fi\": \"fi_core_news_md\",\n",
    "    \"fr\": \"fr_core_news_md\",\n",
    "    \"de\": \"de_core_news_md\",\n",
    "    \"el\": \"el_core_news_md\",\n",
    "    \"it\": \"it_core_news_md\",\n",
    "    \"ja\": \"ja_core_news_md\",\n",
    "    \"ko\": \"ko_core_news_md\",\n",
    "    \"lt\": \"lt_core_news_md\",\n",
    "    \"mk\": \"mk_core_news_md\",\n",
    "    \"nb\": \"nb_core_news_md\",\n",
    "    \"pl\": \"pl_core_news_md\",\n",
    "    \"pt\": \"pt_core_news_md\",\n",
    "    \"ro\": \"ro_core_news_md\",\n",
    "    \"ru\": \"ru_core_news_md\",\n",
    "    \"sl\": \"sl_core_news_md\",\n",
    "    \"es\": \"es_core_news_md\",\n",
    "    \"sv\": \"sv_core_news_md\",\n",
    "    \"uk\": \"uk_core_news_md\"\n",
    "}\n",
    "\n",
    "# fallback option, blank model with language-independent components\n",
    "fallback_model = spacy.blank(\"en\")\n",
    "fallback_model.add_pipe(\"sentencizer\")\n",
    "fallback_model.add_pipe(\"textdescriptives/descriptive_stats\")\n",
    "fallback_model.add_pipe(\"textdescriptives/readability\")\n",
    "fallback_model.add_pipe(\"textdescriptives/quality\")\n",
    "\n",
    "expected_columns = [\n",
    "    \"text\", \"passed_quality_check\", \"n_stop_words\", \"alpha_ratio\", \"mean_word_length\",\n",
    "    \"doc_length\", \"symbol_to_word_ratio_#\", \"proportion_ellipsis\", \"proportion_bullet_points\", \n",
    "    \"contains_lorem ipsum\", \"duplicate_line_chr_fraction\", \"duplicate_paragraph_chr_fraction\", \n",
    "    \"duplicate_ngram_chr_fraction_5\", \"duplicate_ngram_chr_fraction_6\", \"duplicate_ngram_chr_fraction_7\", \n",
    "    \"duplicate_ngram_chr_fraction_8\", \"duplicate_ngram_chr_fraction_9\", \"duplicate_ngram_chr_fraction_10\", \n",
    "    \"top_ngram_chr_fraction_2\", \"top_ngram_chr_fraction_3\", \"top_ngram_chr_fraction_4\", \n",
    "    \"oov_ratio\", \"pos_prop_ADJ\", \"pos_prop_ADP\", \"pos_prop_ADV\", \"pos_prop_AUX\", \n",
    "    \"pos_prop_CCONJ\", \"pos_prop_DET\", \"pos_prop_INTJ\", \"pos_prop_NOUN\", \"pos_prop_NUM\", \n",
    "    \"pos_prop_PART\", \"pos_prop_PRON\", \"pos_prop_PROPN\", \"pos_prop_PUNCT\", \"pos_prop_SCONJ\", \n",
    "    \"pos_prop_SYM\", \"pos_prop_VERB\", \"pos_prop_X\", \"dependency_distance_mean\", \"dependency_distance_std\", \n",
    "    \"prop_adjacent_dependency_relation_mean\", \"prop_adjacent_dependency_relation_std\", \n",
    "    \"entropy\", \"perplexity\", \"per_word_perplexity\", \"first_order_coherence\", \"second_order_coherence\", \n",
    "    \"flesch_reading_ease\", \"flesch_kincaid_grade\", \"smog\", \"gunning_fog\", \"automated_readability_index\", \n",
    "    \"coleman_liau_index\", \"lix\", \"rix\", \"token_length_mean\", \"token_length_median\", \"token_length_std\", \n",
    "    \"sentence_length_mean\", \"sentence_length_median\", \"sentence_length_std\", \"syllables_per_token_mean\", \n",
    "    \"syllables_per_token_median\", \"syllables_per_token_std\", \"n_tokens\", \"n_unique_tokens\", \n",
    "    \"proportion_unique_tokens\", \"n_characters\", \"n_sentences\", \"video_id\", \"language\", \"lang_confidence\", \n",
    "    \"fallback_used\"\n",
    "]\n",
    "\n",
    "\n",
    "# Try to load previously processed data, if it exists\n",
    "try:\n",
    "    processed_df = pd.read_csv(\"processed_text_features.csv\")\n",
    "    processed_video_ids = set(processed_df[\"video_id\"].unique())\n",
    "    print(f\"Resuming from {len(processed_video_ids)} previously processed videos.\")\n",
    "except FileNotFoundError:\n",
    "    processed_video_ids = set()\n",
    "    print(\"Starting fresh, no previously processed videos found.\")\n",
    "\n",
    "def process_transcript(text, lang, video_id, confidence):\n",
    "    \"\"\"Extract text metrics based on the detected language and include Video ID.\"\"\"\n",
    "    if lang in lang2model:\n",
    "        spacy_model = lang2model[lang]\n",
    "        metrics = textdescriptives.extract_metrics(text=text, spacy_model=spacy_model)\n",
    "        is_fallback = False\n",
    "    else:\n",
    "        doc = fallback_model(text)\n",
    "        metrics = textdescriptives.extract_df(doc)\n",
    "        is_fallback = True\n",
    "    \n",
    "    # Add metadata to metrics\n",
    "    metrics[\"video_id\"] = video_id\n",
    "    metrics[\"language\"] = lang\n",
    "    metrics[\"lang_confidence\"] = confidence\n",
    "    metrics[\"fallback_used\"] = is_fallback\n",
    "    return metrics \n",
    "\n",
    "# Iterate over rows in the dataframe\n",
    "for _, row in tqdm(df.iterrows(), total=len(df)):\n",
    "    video_id = row['Video ID']\n",
    "    \n",
    "    # Skip if video has already been processed\n",
    "    if video_id in processed_video_ids:\n",
    "        continue\n",
    "    \n",
    "    # Process transcript \n",
    "    metrics = process_transcript(row['transcript'], row['language'], video_id, row['confidence'])\n",
    "    metrics_df = metrics.reindex(columns=expected_columns)  # Ensure all columns are present\n",
    "    \n",
    "    # Append to the CSV, ensuring headers are written only once\n",
    "    metrics_df.to_csv(\"processed_text_features.csv\", mode=\"a\", header=not bool(processed_video_ids), index=False)\n",
    "\n",
    "    # Add the processed video ID to the set\n",
    "    processed_video_ids.add(video_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd341386-3532-4862-ab47-a78c746b3cd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"processed_text_features.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "21ac34e1-ca21-4fe6-9169-d99aa4d8c00c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 10956 entries, 0 to 10955\n",
      "Data columns (total 74 columns):\n",
      " #   Column                                  Non-Null Count  Dtype  \n",
      "---  ------                                  --------------  -----  \n",
      " 0   text                                    10956 non-null  object \n",
      " 1   passed_quality_check                    10956 non-null  bool   \n",
      " 2   n_stop_words                            10956 non-null  float64\n",
      " 3   alpha_ratio                             10956 non-null  float64\n",
      " 4   mean_word_length                        10956 non-null  float64\n",
      " 5   doc_length                              10956 non-null  float64\n",
      " 6   symbol_to_word_ratio_#                  10956 non-null  float64\n",
      " 7   proportion_ellipsis                     10956 non-null  float64\n",
      " 8   proportion_bullet_points                10956 non-null  float64\n",
      " 9   contains_lorem ipsum                    10956 non-null  float64\n",
      " 10  duplicate_line_chr_fraction             10956 non-null  float64\n",
      " 11  duplicate_paragraph_chr_fraction        10956 non-null  float64\n",
      " 12  duplicate_ngram_chr_fraction_5          10956 non-null  float64\n",
      " 13  duplicate_ngram_chr_fraction_6          10956 non-null  float64\n",
      " 14  duplicate_ngram_chr_fraction_7          10956 non-null  float64\n",
      " 15  duplicate_ngram_chr_fraction_8          10956 non-null  float64\n",
      " 16  duplicate_ngram_chr_fraction_9          10956 non-null  float64\n",
      " 17  duplicate_ngram_chr_fraction_10         10956 non-null  float64\n",
      " 18  top_ngram_chr_fraction_2                10956 non-null  float64\n",
      " 19  top_ngram_chr_fraction_3                10956 non-null  float64\n",
      " 20  top_ngram_chr_fraction_4                10956 non-null  float64\n",
      " 21  oov_ratio                               10844 non-null  float64\n",
      " 22  pos_prop_ADJ                            10956 non-null  float64\n",
      " 23  pos_prop_ADP                            10956 non-null  float64\n",
      " 24  pos_prop_ADV                            10956 non-null  float64\n",
      " 25  pos_prop_AUX                            10956 non-null  float64\n",
      " 26  pos_prop_CCONJ                          10956 non-null  float64\n",
      " 27  pos_prop_DET                            10956 non-null  float64\n",
      " 28  pos_prop_INTJ                           10956 non-null  float64\n",
      " 29  pos_prop_NOUN                           10956 non-null  float64\n",
      " 30  pos_prop_NUM                            10956 non-null  float64\n",
      " 31  pos_prop_PART                           10956 non-null  float64\n",
      " 32  pos_prop_PRON                           10956 non-null  float64\n",
      " 33  pos_prop_PROPN                          10956 non-null  float64\n",
      " 34  pos_prop_PUNCT                          10956 non-null  float64\n",
      " 35  pos_prop_SCONJ                          10956 non-null  float64\n",
      " 36  pos_prop_SYM                            10956 non-null  float64\n",
      " 37  pos_prop_VERB                           10956 non-null  float64\n",
      " 38  pos_prop_X                              10956 non-null  float64\n",
      " 39  dependency_distance_mean                10956 non-null  float64\n",
      " 40  dependency_distance_std                 10956 non-null  float64\n",
      " 41  prop_adjacent_dependency_relation_mean  10956 non-null  float64\n",
      " 42  prop_adjacent_dependency_relation_std   10956 non-null  float64\n",
      " 43  entropy                                 8817 non-null   float64\n",
      " 44  perplexity                              8817 non-null   float64\n",
      " 45  per_word_perplexity                     8817 non-null   float64\n",
      " 46  first_order_coherence                   10789 non-null  float64\n",
      " 47  second_order_coherence                  10702 non-null  float64\n",
      " 48  flesch_reading_ease                     10886 non-null  float64\n",
      " 49  flesch_kincaid_grade                    10886 non-null  float64\n",
      " 50  smog                                    10728 non-null  float64\n",
      " 51  gunning_fog                             10886 non-null  float64\n",
      " 52  automated_readability_index             10956 non-null  float64\n",
      " 53  coleman_liau_index                      10956 non-null  float64\n",
      " 54  lix                                     10956 non-null  float64\n",
      " 55  rix                                     10956 non-null  float64\n",
      " 56  token_length_mean                       10956 non-null  float64\n",
      " 57  token_length_median                     10956 non-null  float64\n",
      " 58  token_length_std                        10956 non-null  float64\n",
      " 59  sentence_length_mean                    10956 non-null  float64\n",
      " 60  sentence_length_median                  10956 non-null  float64\n",
      " 61  sentence_length_std                     10956 non-null  float64\n",
      " 62  syllables_per_token_mean                10886 non-null  float64\n",
      " 63  syllables_per_token_median              10886 non-null  float64\n",
      " 64  syllables_per_token_std                 10886 non-null  float64\n",
      " 65  n_tokens                                10956 non-null  int64  \n",
      " 66  n_unique_tokens                         10956 non-null  int64  \n",
      " 67  proportion_unique_tokens                10956 non-null  float64\n",
      " 68  n_characters                            10956 non-null  int64  \n",
      " 69  n_sentences                             10956 non-null  int64  \n",
      " 70  video_id                                10956 non-null  object \n",
      " 71  language                                10956 non-null  object \n",
      " 72  lang_confidence                         10956 non-null  float64\n",
      " 73  fallback_used                           10956 non-null  bool   \n",
      "dtypes: bool(2), float64(65), int64(4), object(3)\n",
      "memory usage: 6.0+ MB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4acbcce1-00c4-42db-8720-620b9f69e114",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['ar', 'vi', 'cs', 'hi', 'hu', 'th', 'id', 'ta', 'cy', 'nn', 'tr',\n",
       "       'et', 'tl', 'la', 'no', 'he', 'sk', 'ms'], dtype=object)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[df['fallback_used']==1]['language'].unique()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 ",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
